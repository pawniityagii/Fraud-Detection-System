{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d661cd",
   "metadata": {},
   "source": [
    "## 0) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b5b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = 'Fraud.csv'\n",
    "TARGET_COL = 'isFraud'\n",
    "ID_COLS = ['nameOrig','nameDest']\n",
    "TIME_COL = 'step'\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "THRESH_GRID = [i/100 for i in range(1,100)]\n",
    "FN_COST = 500\n",
    "FP_COST = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c39e8",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb53584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, math, os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_OK = True\n",
    "except Exception:\n",
    "    SHAP_OK = False\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LGB_OK = True\n",
    "except Exception:\n",
    "    LGB_OK = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_OK = True\n",
    "except Exception:\n",
    "    XGB_OK = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ccf4c",
   "metadata": {},
   "source": [
    "## 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(CSV_PATH, engine='pyarrow')\n",
    "except Exception:\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "df_shape = df.shape\n",
    "df_head = df.head(3)\n",
    "df_shape, df_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d46beb",
   "metadata": {},
   "source": [
    "## 3) Quick EDA & Leakage Checks\n",
    "Handles missingness overview, class balance, duplicated rows, and potential leak-like columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c826ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ratio = df[TARGET_COL].mean()\n",
    "na_rates = df.isna().mean().sort_values(ascending=False)\n",
    "dup_rows = df.duplicated().sum()\n",
    "leak_like = [c for c in df.columns if c!=TARGET_COL and ('fraud' in c.lower() or 'flag' in c.lower())]\n",
    "display(pd.Series({'rows':df.shape[0],'cols':df.shape[1],'dup_rows':dup_rows,'positive_rate':class_ratio}))\n",
    "display(na_rates.head(15).to_frame('na_rate'))\n",
    "leak_like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5770e",
   "metadata": {},
   "source": [
    "## 4) Feature Engineering\n",
    "Build dense numeric features: deltas, ratios, error terms. Also derive `isMerchantDest` using the merchant naming convention, and one‑hot encode `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85008d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isMerchantDest'] = df['nameDest'].astype(str).str.startswith('M').astype(int) if 'nameDest' in df.columns else 0\n",
    "num_cols = [c for c in ['amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest'] if c in df.columns]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "df['orig_delta'] = df['oldbalanceOrg'] - df['newbalanceOrig'] if set(['oldbalanceOrg','newbalanceOrig']).issubset(df.columns) else np.nan\n",
    "df['dest_delta'] = df['newbalanceDest'] - df['oldbalanceDest'] if set(['oldbalanceDest','newbalanceDest']).issubset(df.columns) else np.nan\n",
    "df['orig_error'] = df['newbalanceOrig'] - (df['oldbalanceOrg'] - df['amount']) if set(['newbalanceOrig','oldbalanceOrg','amount']).issubset(df.columns) else np.nan\n",
    "df['dest_error'] = df['newbalanceDest'] - (df['oldbalanceDest'] + df['amount']) if set(['newbalanceDest','oldbalanceDest','amount']).issubset(df.columns) else np.nan\n",
    "df['amount_over_oldOrg'] = df['amount']/(df['oldbalanceOrg'].replace(0, np.nan)) if set(['amount','oldbalanceOrg']).issubset(df.columns) else np.nan\n",
    "df['amount_over_oldOrg'] = df['amount_over_oldOrg'].replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "cat_cols = [c for c in ['type'] if c in df.columns]\n",
    "drop_cols = [c for c in ID_COLS if c in df.columns]\n",
    "X_cols_base = [c for c in df.columns if c not in drop_cols + [TARGET_COL]]\n",
    "X_cols_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916cfb1",
   "metadata": {},
   "source": [
    "## 5) Data Cleaning — Missing, Outliers, Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['oldbalanceDest','newbalanceDest','oldbalanceOrg','newbalanceOrig','orig_delta','dest_delta','orig_error','dest_error']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(-1)\n",
    "q_low = df['amount'].quantile(0.001) if 'amount' in df.columns else None\n",
    "q_hi = df['amount'].quantile(0.995) if 'amount' in df.columns else None\n",
    "if q_low is not None:\n",
    "    df['amount'] = df['amount'].clip(lower=q_low, upper=q_hi)\n",
    "num_for_vif = [c for c in ['amount','orig_delta','dest_delta','orig_error','dest_error','amount_over_oldOrg'] if c in df.columns]\n",
    "vif_df = []\n",
    "if len(num_for_vif) >= 2:\n",
    "    Xv = df[num_for_vif].replace([np.inf,-np.inf],0).fillna(0).sample(min(500000, len(df)), random_state=RANDOM_STATE)\n",
    "    for i,col in enumerate(num_for_vif):\n",
    "        yv = Xv[col]\n",
    "        Xvv = Xv.drop(columns=[col])\n",
    "        lr = LinearRegression().fit(Xvv, yv)\n",
    "        r2 = lr.score(Xvv, yv)\n",
    "        vif = 1/(1-r2+1e-9)\n",
    "        vif_df.append((col, float(vif)))\n",
    "vif_df = pd.DataFrame(vif_df, columns=['feature','vif']).sort_values('vif', ascending=False)\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbbacc",
   "metadata": {},
   "source": [
    "## 6) Train/Validation Split\n",
    "Time‑aware if `step` exists, else stratified random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696eb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIME_COL in df.columns:\n",
    "    df = df.sort_values(TIME_COL)\n",
    "    cut = int(len(df)*(1-TEST_SIZE))\n",
    "    train_df, val_df = df.iloc[:cut], df.iloc[cut:]\n",
    "else:\n",
    "    train_df, val_df = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL].values\n",
    "y_val = val_df[TARGET_COL].values\n",
    "X_train = train_df.drop(columns=[TARGET_COL] + drop_cols)\n",
    "X_val = val_df.drop(columns=[TARGET_COL] + drop_cols)\n",
    "num_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_features = [c for c in cat_cols if c in X_train.columns]\n",
    "pre = ColumnTransformer([\n",
    "    ('num','passthrough', num_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)\n",
    "], remainder='drop')\n",
    "X_train.shape, X_val.shape, len(num_features), len(cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beff6b9",
   "metadata": {},
   "source": [
    "## 7) Models\n",
    "Logistic Regression (interpretable), Gradient Boosting (strong baseline), and optional LightGBM/XGBoost if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f2076",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('logreg', Pipeline([('pre', pre), ('clf', LogisticRegression(max_iter=200, class_weight='balanced', n_jobs=None, solver='lbfgs'))])))\n",
    "models.append(('hgb', Pipeline([('pre', pre), ('clf', HistGradientBoostingClassifier(max_depth=None, learning_rate=0.1, max_iter=200, random_state=RANDOM_STATE))])))\n",
    "if LGB_OK:\n",
    "    models.append(('lgbm', Pipeline([('pre', pre), ('clf', LGBMClassifier(random_state=RANDOM_STATE, n_estimators=500, learning_rate=0.05, objective='binary'))])))\n",
    "if XGB_OK:\n",
    "    models.append(('xgb', Pipeline([('pre', pre), ('clf', XGBClassifier(random_state=RANDOM_STATE, n_estimators=500, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, objective='binary:logistic', eval_metric='auc'))])))\n",
    "results = []\n",
    "for name, pipe in models:\n",
    "    pipe.fit(X_train, y_train)\n",
    "    p_val = pipe.predict_proba(X_val)[:,1]\n",
    "    roc = roc_auc_score(y_val, p_val)\n",
    "    pr = average_precision_score(y_val, p_val)\n",
    "    results.append((name, roc, pr, p_val, pipe))\n",
    "res_df = pd.DataFrame([(n, r, p) for n,r,p,_,_ in results], columns=['model','roc_auc','pr_auc']).sort_values(['pr_auc','roc_auc'], ascending=False)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca69a21",
   "metadata": {},
   "source": [
    "## 8) Curves: ROC and Precision‑Recall (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3208b6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best = sorted(results, key=lambda x: (x[2], x[1]), reverse=True)[0]\n",
    "best_name, best_roc, best_pr, best_pval, best_pipe = best\n",
    "fpr, tpr, thr = roc_curve(y_val, best_pval)\n",
    "plt.figure(); plt.plot(fpr, tpr); plt.plot([0,1],[0,1]); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC: '+best_name)\n",
    "pr, rc, thr2 = precision_recall_curve(y_val, best_pval)\n",
    "plt.figure(); plt.plot(rc, pr); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('PR: '+best_name)\n",
    "best_name, best_roc, best_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bc1d6",
   "metadata": {},
   "source": [
    "## 9) Cost‑Sensitive Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e397e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def eval_threshold(y_true, p, t):\n",
    "    yhat = (p>=t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, yhat).ravel()\n",
    "    cost = fp*FP_COST + fn*FN_COST\n",
    "    return {'t':t,'tp':int(tp),'fp':int(fp),'tn':int(tn),'fn':int(fn),'precision': tp/max(1,tp+fp), 'recall': tp/max(1,tp+fn), 'f1': (2*tp)/max(1,2*tp+fp+fn), 'cost': cost}\n",
    "grid = [eval_threshold(y_val, best_pval, t) for t in THRESH_GRID]\n",
    "gdf = pd.DataFrame(grid).sort_values('cost')\n",
    "t_star = float(gdf.iloc[0]['t'])\n",
    "star = gdf.iloc[0].to_dict()\n",
    "plt.figure(); plt.plot(gdf['t'], gdf['cost']); plt.xlabel('Threshold'); plt.ylabel('Expected Cost'); plt.title('Cost vs Threshold')\n",
    "star, t_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e160272",
   "metadata": {},
   "source": [
    "## 10) Final Validation Metrics at Cost‑Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7891b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yhat_star = (best_pval>=t_star).astype(int)\n",
    "cm = confusion_matrix(y_val, yhat_star)\n",
    "report = classification_report(y_val, yhat_star, output_dict=False)\n",
    "plt.figure();\n",
    "plt.imshow(cm, interpolation='nearest'); plt.title('Confusion Matrix'); plt.xlabel('Predicted'); plt.ylabel('Actual');\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a4725",
   "metadata": {},
   "source": [
    "## 11) Feature Importance & Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc158b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = []\n",
    "num_features = [c for c in X_train.columns if c in best_pipe.named_steps['pre'].transformers_[0][2]]\n",
    "cat_features = best_pipe.named_steps['pre'].transformers_[1][2] if len(best_pipe.named_steps['pre'].transformers_)>1 else []\n",
    "ohe = best_pipe.named_steps['pre'].named_transformers_['cat'] if len(cat_features)>0 else None\n",
    "if ohe is not None:\n",
    "    ohe_names = ohe.get_feature_names_out(cat_features).tolist()\n",
    "else:\n",
    "    ohe_names = []\n",
    "feature_names = num_features + ohe_names\n",
    "try:\n",
    "    if SHAP_OK and hasattr(best_pipe.named_steps['clf'], 'predict_proba'):\n",
    "        Xs = best_pipe.named_steps['pre'].transform(X_val.sample(min(10000,len(X_val)), random_state=RANDOM_STATE))\n",
    "        explainer = shap.Explainer(best_pipe.named_steps['clf'])\n",
    "        sv = explainer(Xs)\n",
    "        shap.plots.beeswarm(sv, max_display=20)\n",
    "except Exception as e:\n",
    "    pass\n",
    "if not SHAP_OK:\n",
    "    Xs = X_val.sample(min(5000, len(X_val)), random_state=RANDOM_STATE)\n",
    "    perm = permutation_importance(best_pipe, Xs, y_val[:len(Xs)], n_repeats=3, random_state=RANDOM_STATE)\n",
    "    imp = pd.Series(perm.importances_mean, index=feature_names).sort_values(ascending=False)[:20]\n",
    "    plt.figure(); imp.iloc[:20].plot(kind='bar'); plt.title('Permutation Importance (Top 20)'); plt.tight_layout()\n",
    "    imp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c86ab",
   "metadata": {},
   "source": [
    "## 12) Variable Selection Strategy\n",
    "1) Start with domain‑derived engineered features (deltas, errors, ratios). 2) Encode `type`. 3) Remove identifiers. 4) Winsorize extreme `amount`. 5) Examine VIF to avoid redundant numeric features. 6) Keep features that improve PR‑AUC and reduce cost on validation. 7) Prefer parsimonious sets if two variants tie on cost.\n",
    "\n",
    "The next cell runs a compact forward‑style selection on top of the preprocessor to confirm robustness for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f995d0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_feats = [c for c in ['amount','isMerchantDest','amount_over_oldOrg'] if c in X_train.columns]\n",
    "cand_feats = [c for c in X_train.columns if c not in base_feats]\n",
    "sel = set(base_feats)\n",
    "best_cost = None\n",
    "history = []\n",
    "for c in cand_feats:\n",
    "    cols = list(sel | {c})\n",
    "    num_f = [f for f in cols if f not in ['type'] and X_train[f].dtype!='O']\n",
    "    cat_f = ['type'] if 'type' in cols else []\n",
    "    pre2 = ColumnTransformer([('num','passthrough',num_f), ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_f)], remainder='drop')\n",
    "    pipe2 = Pipeline([('pre', pre2), ('clf', HistGradientBoostingClassifier(max_iter=200, random_state=RANDOM_STATE))])\n",
    "    pipe2.fit(X_train[cols], y_train)\n",
    "    p = pipe2.predict_proba(X_val[cols])[:,1]\n",
    "    g = [eval_threshold(y_val, p, t) for t in THRESH_GRID]\n",
    "    cmin = min(g, key=lambda d: d['cost'])['cost']\n",
    "    history.append((c, cmin))\n",
    "    if best_cost is None or cmin < best_cost:\n",
    "        best_cost = cmin\n",
    "        sel.add(c)\n",
    "sel, sorted(history, key=lambda x: x[1])[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa8e8c",
   "metadata": {},
   "source": [
    "## 13) Export Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "MODEL_PATH = '/mnt/data/fraud_model.joblib'\n",
    "joblib.dump(best_pipe, MODEL_PATH)\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96a066",
   "metadata": {},
   "source": [
    "## 14) Actionable Insights — Key Drivers\n",
    "- Large `amount` relative to `oldbalanceOrg`\n",
    "- Mismatch in balance equations via `orig_error` and `dest_error`\n",
    "- Destination merchant indicator `isMerchantDest`\n",
    "- Transaction `type` patterns (e.g., TRANSFER → CASH_OUT)\n",
    "\n",
    "These align with known fraud modus operandi where compromised accounts attempt rapid fund transfer and cash‑out, often to merchant‑like accounts.\n",
    "\n",
    "Use the importance/SHAP view above to tailor the final narrative to your fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8204b00",
   "metadata": {},
   "source": [
    "## 15) Prevention Recommendations\n",
    "- Real‑time holds for high‑risk scores at threshold *t★*\n",
    "- Velocity checks over `step` window: cumulative amount and count by `nameOrig` and `nameDest`\n",
    "- Step‑up authentication for risky `type` flows and first‑time counterparties\n",
    "- Dest account risk graph: prior fraud adjacency, merchant patterns, sudden degree spikes\n",
    "- Adaptive limits: per‑user dynamic cap tied to behavioral baseline\n",
    "- Periodic model recalibration and concept‑drift monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbd1aa",
   "metadata": {},
   "source": [
    "## 16) How To Measure Impact\n",
    "- Launch as shadow mode, then phased A/B rollout\n",
    "- Primary KPIs: fraud loss, recovery rate, manual review load, customer friction rate\n",
    "- Secondary KPIs: model precision at *t★*, recall, PR‑AUC; false‑positive appeal rate\n",
    "- Use interleaved evaluation across time bands to avoid step‑based drift leakage\n",
    "- Weekly drift dashboard on score distribution and population stability index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
